<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- <title>Reviewed Articles - [Your Name]</title> -->
  <link rel="stylesheet" href="styles.css" />
  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
</head>
<body>
  <!-- Navigation Bar -->
  <header>
    <div class="container">
      <!-- Your Name -->
      <div class="header-top">
        <!-- <h1>Johannes Walter</h1> -->
      </div>
      <!-- Navigation Bar -->
      <nav class="navbar">
        <ul class="nav-links">
          <li><a href="index.html">Home</a></li>
          <li><a href="articles.html">Research</a></li>
          <!-- <li><a href="contact.html">Contact Information</a></li> -->
          <li><a href="CV.pdf">CV</a></li>
        </ul>
        <a href="#" id="darkModeToggle" title="Toggle Dark Mode">
          <i class="fas fa-lightbulb"></i>
          <span class="mode-text">Dark Mode</span>
        </a>
      </nav>
    </div>
  </header>

  <!-- Main Content -->
  <main class="container">
    
    <ul class="paper-list">
      <li>
        <a href="paper1.pdf" target="_blank">Using AI Persuasion to Reduce Political
            Polarization</a>
        <p>Job Market Paper</p>
        <br>
        <p>Political polarization is a growing problem in democratic societies, negatively affect‐
            ing everything from personal behavior to the functioning of institutions. This paper investigates a new
            way to reduce polarization: AI‐powered persuasion. In a pre‐registered randomized
            controlled trial with a representative sample of the US population, I show that conversational AI agents can persuade some people to adopt more moderate views on the
            issue of U.S. support for Ukraine. As a result, overall ideological polarization in the
            sample is reduced by about 20 percentage points. This depolarization effect was still
            present in an obfuscated follow‐up study one month later. These findings suggest that
            AI‐powered persuasion could be a useful tool in efforts to reduce polarization.</p>
        <br>
        <div class="links">
          <a href="paper1.pdf" target="_blank">[paper]</a>
          <a href="code1.zip" target="_blank">[code]</a>
          <a href="data1.zip" target="_blank">[data]</a>
          <a href="preregistration1.pdf" target="_blank">[preregistration]</a>
          <a href="ethical_approval1.pdf" target="_blank">[ethical approval]</a>
        </div>
      </li>
      <br><br /><br><br />
      <li>
        <a href="article2.pdf" target="_blank">Advised by an Algorithm: Learning with Different
            Informational Resources and Reactions to Heterogeneous
            Advice Quality</a>
        <p>Joint work with Jan Biermann and John Horton.</p>
        <br>
        <p>In a wide range of settings, decision-makers increasingly rely on algorithmic tools for
            support. Often, the algorithm serves as an advisor, leaving the final decision to be made
            by human judgment. In this setting, we focus on two aspects: first, identifying the informational
            resources that aid individuals in evaluating algorithmic guidance, and second,
            exploring human reactions to varying qualities of algorithmic advice. To address these
            questions, we conducted an online experiment involving 1565 participants. In the baseline
            treatment, subjects repeatedly perform the same estimation task and are provided with
            algorithmic guidance, all without knowledge of the type of algorithm or feedback after
            each round. Subsequently, we introduce two interventions aimed at enhancing the quality
            of human decisions when receiving algorithmic advice. In the first intervention, we explain
            the way the algorithm functions. We find that while this intervention reduces adherence
            to algorithmic advice, it does not improve decision-making performance. In the second
            treatment, we disclose the correct answer to the task after each round. This intervention
            leads to a reduction in adherence to algorithmic advice and an improvement in human
            decision-making performance. Furthermore, we investigate the extent to which individuals
            can adjust their assessment of the algorithm when advice quality fluctuates due to
            external circumstances. We find some evidence that individuals can assess algorithmic
            advice thoughtfully, adjusting their adherence depending on the quality of algorithmic
            recommendations.</p>
        <br>
        <div class="links">
          <a href="article2.pdf" target="_blank">[paper]</a>
          <a href="code2.zip" target="_blank">[code]</a>
          <a href="data2.zip" target="_blank">[data]</a>
          <a href="preregistration2.pdf" target="_blank">[preregistration]</a>
          <a href="ethical_approval2.pdf" target="_blank">[ethical approval]</a>
        </div>
      </li>
      <br><br /><br><br />
      <li>
        <a href="article2.pdf" target="_blank">The Effect of AI on the demand for Human Expertise</a>
        <p>Joint work with Sebastian Valet.</p>
        <br>
        <p>This paper investigates the impact of consumer AI adoption, specifically ChatGPT, 
            on the demand for human expertise. Utilizing a dual methodology, we first analyze extensive 
            observational data from over 100,000 users to assess the downstream effects of ChatGPT adoption. 
            Our findings indicate a significant reduction in visits to websites offering human expertise, 
            such as WebMD and Quora, following AI adoption. In the second part of our study, we conduct an 
            online lab experiment to explore the underlying mechanisms. Participants are tasked with finding 
            information about a disease they currently have, with three groups: one interacting with an AI, 
            another using online search, and a control group with no assistance. The outcome measured is the 
            self-reported likelihood of visiting a doctor. Results show that participants in the AI treatment 
            group report a significantly lower probability of seeking medical advice compared to the online 
            search and control groups. These findings suggest that AI expertise may crowd out human knowledge, 
            with profound implications for regulation and the labor market. </p>
        <br>
        <div class="links">
          <a href="article2.pdf" target="_blank">[paper]</a>
          <a href="code2.zip" target="_blank">[code]</a>
          <a href="data2.zip" target="_blank">[data]</a>
          <a href="preregistration2.pdf" target="_blank">[preregistration]</a>
          <a href="ethical_approval2.pdf" target="_blank">[ethical approval]</a>
        </div>
      </li>
      <br><br /><br><br />
      <li>
        <a href="article2.pdf" target="_blank">Turing Markets</a>
        <p>Joint work with Dominik Rehse and Sebastian Valet.</p>
        <br>
        <p>Abstract: ??</p>
        <br>
        <div class="links">
          <a href="article2.pdf" target="_blank">[paper]</a>
          <a href="code2.zip" target="_blank">[code]</a>
          <a href="data2.zip" target="_blank">[data]</a>
          <a href="preregistration2.pdf" target="_blank">[preregistration]</a>
          <a href="ethical_approval2.pdf" target="_blank">[ethical approval]</a>
        </div>
      </li>
      <br><br /><br><br />
      <li>
        <a href="article2.pdf" target="_blank">Incentives and Economics of Data Sharing</a>
        <p>Joint work with Dominik Rehse and Sebastian Valet.</p>
        <br>
        <p>Abstract: ??</p>
        <br>
        <div class="links">
          <a href="article2.pdf" target="_blank">[paper]</a>
          <a href="code2.zip" target="_blank">[code]</a>
          <a href="data2.zip" target="_blank">[data]</a>
          <a href="preregistration2.pdf" target="_blank">[preregistration]</a>
          <a href="ethical_approval2.pdf" target="_blank">[ethical approval]</a>
        </div>
      </li>
      <!-- Add more articles as needed -->
    </ul>
  </main>

  <!-- Footer -->
  <footer>
    <div class="container">
        <p>&copy; 2025 Johannes Walter. Content available under 
            <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener noreferrer">
                CC BY-NC 4.0
            </a> unless otherwise noted.
        </p>
    </div>
</footer>

<script src="script.js"></script>
</body>
</html> 